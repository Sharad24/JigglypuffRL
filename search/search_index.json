{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JigglypuffRL An open source Reinforement Learning library based on Pytorch KeyFeatures & Capabilities Clean Interface following Pytorch Reproducibility Support in terms of tools across the whole spectrum of RL research Extensive Documentation Getting Started Installation We suggest creating a conda virtual environment before installing our package. conda create -n JgRL python=3.6 pip conda activate JgRL From Source (Recommended) git clone https://github.com/SforAiDl/JigglypuffRL.git cd JigglypuffRL pip install -r requirements.txt python setup.py install Using Pip pip install jigglypuff-rl # for most recent stable release pip install jigglypuff-rl==0.0.1dev2 # for most recent development release","title":"JigglypuffRL"},{"location":"#jigglypuffrl","text":"An open source Reinforement Learning library based on Pytorch","title":"JigglypuffRL"},{"location":"#keyfeatures-capabilities","text":"Clean Interface following Pytorch Reproducibility Support in terms of tools across the whole spectrum of RL research Extensive Documentation","title":"KeyFeatures &amp; Capabilities"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#installation","text":"We suggest creating a conda virtual environment before installing our package. conda create -n JgRL python=3.6 pip conda activate JgRL","title":"Installation"},{"location":"#from-source-recommended","text":"git clone https://github.com/SforAiDl/JigglypuffRL.git cd JigglypuffRL pip install -r requirements.txt python setup.py install","title":"From Source (Recommended)"},{"location":"#using-pip","text":"pip install jigglypuff-rl # for most recent stable release pip install jigglypuff-rl==0.0.1dev2 # for most recent development release","title":"Using Pip"},{"location":"installation/","text":"Installation We suggest creating a conda virtual environment before installing our package. conda create -n JgRL_env python=3.6 pip conda activate JgRL_env From Source (Recommended) git clone https://github.com/SforAiDl/JigglypuffRL.git cd JigglypuffRL pip install -r requirements.txt python setup.py install Using Pip pip install jigglypuff-rl # for most recent stable release pip install jigglypuff-rl==0.0.1dev2 # for most recent development release","title":"Installation"},{"location":"installation/#installation","text":"We suggest creating a conda virtual environment before installing our package. conda create -n JgRL_env python=3.6 pip conda activate JgRL_env","title":"Installation"},{"location":"installation/#from-source-recommended","text":"git clone https://github.com/SforAiDl/JigglypuffRL.git cd JigglypuffRL pip install -r requirements.txt python setup.py install","title":"From Source (Recommended)"},{"location":"installation/#using-pip","text":"pip install jigglypuff-rl # for most recent stable release pip install jigglypuff-rl==0.0.1dev2 # for most recent development release","title":"Using Pip"},{"location":"classicalrl/bandit/bandit/","text":"Bandit Algorithms JigglypuffRL currently supports multi-armed Bandits with the every arm modeled as: - Bernoulli/Beta Random Variable - Gaussian Random Variable There current list of supported Bandit Algorithms are: 1. Epsilon Greedy 2. Upper Confidence Bound 3. Bayesian Upper Confidence Bound (only in Bernoulli/Beta RV Formulation) 4. Thompson Sampling (only in Bernoulli/Beta RV formulation) 5. Softmax Action Selection (only in Gaussian RV formulation) An example of the usage is as follows: Import from jigglypuffRL import EpsGreedyGaussianBandit, EpsGreedyBernoulliBandit, \\ UCBGaussianBandit, UCBBernoulliBandit, BayesianUCBBernoulliBandit, \\ ThompsonSampling, SoftmaxAction Selection Initialize Models epsGreedyBandit = EpsGreedyGaussianBandit(bandits=1, arms=10, eps=0.05) ucbBandit = UCBGaussianBandit(bandits=1, arms=10) softmaxBandit = SoftmaxActionSelection(bandits=1, arms=10) Learn epsGreedyBandit.learn(n_timesteps=1000) ucbBandit.learn(1000) softmaxBandit.learn(1000) Plot import matplotlib.pyplot as plt plt.plot(epsGreedyBandit.regrets, label=\"eps greedy\") plt.plot(ucbBandit.regrets, label=\"ucb\") plt.plot(softmaxBandit.regrets, label=\"softmax\") plt.legend() plt.savefig(\"GuassianBanditsRegret.png\") plt.cla() Initialize Bernoulli/Beta models epsbernoulli = EpsGreedyBernoulliBandit(bandits=1, arms=10, eps=0.05) ucbbernoulli = UCBBernoulliBandit(bandits=1, arms=10) thsampling = ThompsonSampling(bandits=1, arms=10, a=1, b=1) bayesianbandit = BayesianUCBBernoulliBandit(bandits=1, arms=10, a=1, b=1, c=3) Learn epsbernoulli.learn(1000) ucbbernoulli.learn(1000) thsampling.learn(1000) bayesianbandit.learn(1000) Plot plt.plot(epsbernoulli.regrets, label=\"eps\") plt.plot(ucbbernoulli.regrets, label=\"ucb\") plt.plot(bayesianbandit.regrets, label=\"Bayesian UCB\") plt.plot(thsampling.regrets, label=\"Thompson Sampling\") plt.legend() plt.savefig(\"BernoulliBanditsRegret.png\") This creates the following plots: Gaussian Regrets Bernoulli Regrets","title":"Bandit Algorithms"},{"location":"classicalrl/bandit/bandit/#bandit-algorithms","text":"JigglypuffRL currently supports multi-armed Bandits with the every arm modeled as: - Bernoulli/Beta Random Variable - Gaussian Random Variable There current list of supported Bandit Algorithms are: 1. Epsilon Greedy 2. Upper Confidence Bound 3. Bayesian Upper Confidence Bound (only in Bernoulli/Beta RV Formulation) 4. Thompson Sampling (only in Bernoulli/Beta RV formulation) 5. Softmax Action Selection (only in Gaussian RV formulation) An example of the usage is as follows:","title":"Bandit Algorithms"},{"location":"classicalrl/bandit/bandit/#import","text":"from jigglypuffRL import EpsGreedyGaussianBandit, EpsGreedyBernoulliBandit, \\ UCBGaussianBandit, UCBBernoulliBandit, BayesianUCBBernoulliBandit, \\ ThompsonSampling, SoftmaxAction Selection","title":"Import"},{"location":"classicalrl/bandit/bandit/#initialize-models","text":"epsGreedyBandit = EpsGreedyGaussianBandit(bandits=1, arms=10, eps=0.05) ucbBandit = UCBGaussianBandit(bandits=1, arms=10) softmaxBandit = SoftmaxActionSelection(bandits=1, arms=10)","title":"Initialize Models"},{"location":"classicalrl/bandit/bandit/#learn","text":"epsGreedyBandit.learn(n_timesteps=1000) ucbBandit.learn(1000) softmaxBandit.learn(1000)","title":"Learn"},{"location":"classicalrl/bandit/bandit/#plot","text":"import matplotlib.pyplot as plt plt.plot(epsGreedyBandit.regrets, label=\"eps greedy\") plt.plot(ucbBandit.regrets, label=\"ucb\") plt.plot(softmaxBandit.regrets, label=\"softmax\") plt.legend() plt.savefig(\"GuassianBanditsRegret.png\") plt.cla()","title":"Plot"},{"location":"classicalrl/bandit/bandit/#initialize-bernoullibeta-models","text":"epsbernoulli = EpsGreedyBernoulliBandit(bandits=1, arms=10, eps=0.05) ucbbernoulli = UCBBernoulliBandit(bandits=1, arms=10) thsampling = ThompsonSampling(bandits=1, arms=10, a=1, b=1) bayesianbandit = BayesianUCBBernoulliBandit(bandits=1, arms=10, a=1, b=1, c=3)","title":"Initialize Bernoulli/Beta models"},{"location":"classicalrl/bandit/bandit/#learn_1","text":"epsbernoulli.learn(1000) ucbbernoulli.learn(1000) thsampling.learn(1000) bayesianbandit.learn(1000)","title":"Learn"},{"location":"classicalrl/bandit/bandit/#plot_1","text":"plt.plot(epsbernoulli.regrets, label=\"eps\") plt.plot(ucbbernoulli.regrets, label=\"ucb\") plt.plot(bayesianbandit.regrets, label=\"Bayesian UCB\") plt.plot(thsampling.regrets, label=\"Thompson Sampling\") plt.legend() plt.savefig(\"BernoulliBanditsRegret.png\") This creates the following plots: Gaussian Regrets Bernoulli Regrets","title":"Plot"}]}